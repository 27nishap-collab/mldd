{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y rdkit-pypi numpy\n",
        "!pip install rdkit-pypi"
      ],
      "metadata": {
        "id": "WV-HATB_Exll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.4"
      ],
      "metadata": {
        "id": "9rtf4DLV9GuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors, AllChem, DataStructs\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from rdkit import RDLogger\n",
        "RDLogger.DisableLog('rdApp.*')\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "train_df = pd.read_csv('/content/drive/Shared drives/MLDD/ORD_Data/Main Data/train_04_13_25.csv')\n",
        "test_df =  pd.read_csv('/content/drive/Shared drives/MLDD/ORD_Data/Main Data/test_04_13_25.csv')\n",
        "\n",
        "train_df['data_type'] = 'Train'\n",
        "test_df['data_type'] = 'Test'\n",
        "allData = pd.concat([train_df, test_df], axis=0)\n",
        "allData.head(2)"
      ],
      "metadata": {
        "id": "G8YAQ3KM_w9D",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allData.shape"
      ],
      "metadata": {
        "id": "Z7WTF_tsFd0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = allData.copy()\n",
        "data = data[data['Yield'] < 100]\n",
        "data = data[data['Yield'] != 0]\n",
        "data = data.replace('None', np.nan) #should consider adding\n",
        "data = data.dropna(subset=['Yield'])"
      ],
      "metadata": {
        "id": "l2t8wFPA_hds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFNGb3UuCYzd"
      },
      "outputs": [],
      "source": [
        "if 'Temperature' in data.columns and 'Time' in data.columns:\n",
        "    data['TempTimeInteraction'] = data['Temperature'] * data['Time'] #add\n",
        "\n",
        "# RDKit functions\n",
        "def molFromSmiles(smiles):\n",
        "    try:\n",
        "        return Chem.MolFromSmiles(smiles)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def calcDesc(mol):\n",
        "    if mol is None:\n",
        "        return [np.nan]*6\n",
        "    return [\n",
        "        Descriptors.MolWt(mol),\n",
        "        Descriptors.MolLogP(mol),\n",
        "        Descriptors.TPSA(mol),\n",
        "        Descriptors.NumHDonors(mol),   #hdonor\n",
        "        Descriptors.NumHAcceptors(mol),    #numh acceptors\n",
        "        Descriptors.NumRotatableBonds(mol)   #rot bonds\n",
        "    ]\n",
        "\n",
        "def morgan_fp(smiles, n_bits=128):    #smaller nbits number? finetune this?\n",
        "    mol = molFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return np.zeros(n_bits)\n",
        "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=n_bits)\n",
        "    arr = np.zeros((n_bits,), dtype=int)\n",
        "    DataStructs.ConvertToNumpyArray(fp, arr)\n",
        "    return arr\n",
        "\n",
        "# Feature definitions\n",
        "descNames = ['MolWt', 'LogP', 'TPSA', 'HDonors', 'HAcceptors', 'RotBonds']\n",
        "smilesRoles = [\n",
        "    ('COOH SMILES', 'COOH'),\n",
        "    ('Amine SMILES', 'Amine'),\n",
        "    ('Additive SMILES', 'Additive'),\n",
        "    ('Coupling Agent SMILES', 'Coupling Agent'),\n",
        "    ('Solvent SMILES', 'Solvent')\n",
        "]\n",
        "numericalFeatures = [\n",
        "    'Temperature', 'Time', 'TempTimeInteraction',\n",
        "    'COOH MW', 'COOH logP', 'Amine MW', 'Amine logP'  #, 'data_type'\n",
        "]\n",
        "categoricalFeatures = [\n",
        "    'Solvent', 'Coupling Agent', 'COOH', 'Amine', 'Additive'\n",
        "]\n",
        "yield_split_features = ['Yield', 'data_type']\n",
        "\n",
        "# Feature engineering for GBR, SVR, RF, LGBM\n",
        "def process_smiles_features(data, fp_bits=128):\n",
        "    for smilesCol, prefix in smilesRoles:\n",
        "        if smilesCol in data.columns:\n",
        "            molCol = f'{prefix}_Mol'   #f syntax GOT IT\n",
        "            data[molCol] = data[smilesCol].apply(molFromSmiles) #apply feature pandas function\n",
        "            descDf = data[molCol].apply(calcDesc).apply(pd.Series)\n",
        "            descDf.columns = [f'{prefix}_{n}' for n in descNames]  #list comprehension: renaming elements in descnames\n",
        "            data = pd.concat([data, descDf], axis=1)\n",
        "            data = data.drop(columns=[molCol]) #removes the molCol column (saving objects we no longer need)\n",
        "            for n in descNames:\n",
        "                col = f'{prefix}_{n}'\n",
        "                if col in data.columns and col not in numericalFeatures:\n",
        "                    numericalFeatures.append(col)\n",
        "\n",
        "            fps = data[smilesCol].fillna('').apply(lambda s: morgan_fp(s, n_bits=fp_bits)) #lambda functions in pandas\n",
        "            fp_df = pd.DataFrame(fps.tolist(), columns=[f'{prefix}_FP_{i}' for i in range(fp_bits)], index=data.index) #how fun\n",
        "            data = pd.concat([data, fp_df], axis=1)\n",
        "            for i in range(fp_bits):\n",
        "                col = f'{prefix}_FP_{i}'\n",
        "                if col in data.columns and col not in numericalFeatures:\n",
        "                    numericalFeatures.append(col)\n",
        "\n",
        "#what in the world!! read up on this.\n",
        "            vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 4), min_df=1, max_features=32)\n",
        "            smiles_vec = vectorizer.fit_transform(data[smilesCol].fillna('')).toarray()     #toarray function.  numpy courses?\n",
        "            smiles_vec_names = [f\"{prefix}_SMILES_{f}\" for f in vectorizer.get_feature_names_out()]\n",
        "            smiles_vec_df = pd.DataFrame(smiles_vec, columns=smiles_vec_names, index=data.index)\n",
        "            data = pd.concat([data, smiles_vec_df], axis=1)\n",
        "            for name in smiles_vec_names:\n",
        "                if name not in numericalFeatures:\n",
        "                    numericalFeatures.append(name)\n",
        "    return data\n",
        "\n",
        "# Prepare data for models\n",
        "data_gbr_rf_lgbm = process_smiles_features(data.copy(), fp_bits=128)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Common preprocessing function\n",
        "def preprocess_data(X, y, scaler_type='standard'):\n",
        "    X = pd.get_dummies(X, columns=[col for col in X.columns if X[col].dtype == 'object'], dummy_na=False)\n",
        "    tmp = X.columns\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    X = imputer.fit_transform(X)\n",
        "    if scaler_type == 'robust':\n",
        "        scaler = RobustScaler()\n",
        "    else:\n",
        "        scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    X = pd.DataFrame(X, columns=tmp)\n",
        "    return X, y\n"
      ],
      "metadata": {
        "id": "M2UINA7u0Em5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "availableNumericalFeatures = [col for col in numericalFeatures if col in data_gbr_rf_lgbm.columns]\n",
        "availableCategoricalFeatures = [col for col in categoricalFeatures if col in data_gbr_rf_lgbm.columns]"
      ],
      "metadata": {
        "id": "dv_G5Kkc0DE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitFeature = data_gbr_rf_lgbm['data_type'] #pandas series"
      ],
      "metadata": {
        "id": "JimBAr0-6wz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_rf = data_gbr_rf_lgbm[availableNumericalFeatures + availableCategoricalFeatures]\n",
        "\n",
        "# + splitFeature\n",
        "\n",
        "y_rf = data_gbr_rf_lgbm[yield_split_features]\n",
        "X_rf, y_rf = preprocess_data(X_rf, y_rf)"
      ],
      "metadata": {
        "id": "XN-eN6UQIBlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_rf = X_rf.reset_index()"
      ],
      "metadata": {
        "id": "bZ0BeznSCDbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitFeature = splitFeature.reset_index()"
      ],
      "metadata": {
        "id": "xAeqkfLsCG8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_rf['data_type'] = splitFeature['data_type']"
      ],
      "metadata": {
        "id": "G02WovRzCuNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_rf['data_type'] = splitFeature['data_type']\n",
        "# X_rf = X_rf.drop(columns=['index'])"
      ],
      "metadata": {
        "id": "sg25tlHw2ryW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train = X_rf[X_rf['data_type'] == 'Train']\n",
        "# y_train = y_rf[y_rf['data_type'] == 'Train']['Yield']\n",
        "# X_test = X_rf[X_rf['data_type'] == 'Test']\n",
        "# y_test = y_rf[y_rf['data_type'] == 'Test']['Yield']\n",
        "\n",
        "# X_train = X_train.drop(columns=['data_type'])\n",
        "# X_test = X_test.drop(columns=['data_type'])\n",
        "\n",
        "# rf = RandomForestRegressor(\n",
        "#     n_estimators=500, max_depth=20, min_samples_leaf=3, n_jobs=-1, random_state=42\n",
        "# )\n",
        "# rf.fit(X_train, y_train)\n",
        "# yPred_rf = rf.predict(X_test)\n",
        "# r2_rf = r2_score(y_test, yPred_rf)\n",
        "# # print(f\"Random Forest r^2: {r2_rf:.4f}\")"
      ],
      "metadata": {
        "id": "M9YyB8ZwAZ_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9245ea88"
      },
      "source": [
        "X_train = X_rf[X_rf['data_type'] == 'Train'].drop(columns=['data_type'])\n",
        "y_train = y_rf[y_rf['data_type'] == 'Train']['Yield']\n",
        "X_test = X_rf[X_rf['data_type'] == 'Test'].drop(columns=['data_type'])\n",
        "y_test = y_rf[y_rf['data_type'] == 'Test']['Yield']\n",
        "\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=500, max_depth=20, min_samples_leaf=3, n_jobs=-1, random_state=42\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "yPred_rf = rf.predict(X_test)\n",
        "r2_rf = r2_score(y_test, yPred_rf)\n",
        "print(f\"Random Forest r^2: {r2_rf:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "\n",
        "mae = mean_absolute_error(y_test, yPred_rf)\n",
        "print(f\"Degree MAE: {mae}\")\n"
      ],
      "metadata": {
        "id": "CP1NywZPUjMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_train.plot.kde(bw_method=0.3, label='KDE Plot', color='blue')\n",
        "\n",
        "plt.title('Kernel Density Estimation Plot')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uPLfIt-jVWwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7112f2ed"
      },
      "source": [
        "The previous error was caused by a conflict between the numpy version installed by `rdkit-pypi` and the numpy version explicitly installed later. To resolve this, I've uninstalled both `rdkit-pypi` and `numpy`, and then reinstalled `rdkit-pypi`. This ensures that a compatible version of numpy is installed alongside `rdkit-pypi`."
      ]
    }
  ]
}